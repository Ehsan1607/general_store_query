import os
import pandas as pd
import pickle
from modules.inventory import load_inventory
from modules.query_processing import interpret_query, determine_department, correct_spelling
from modules.embedding import create_or_load_faiss_index, embed_query, generate_user_response
from config.config import get_openai_api_key

# Step 1: Load the inventory from a CSV file
inventory_file = "inventory.csv"
inventory = load_inventory(inventory_file)

# Step 2: Create or load the FAISS index for embedding-based search
index, embedding_model = create_or_load_faiss_index(inventory, "faiss_index/index.bin")

# Load embedding cache if available
embedding_cache_file = "embedding_cache.pkl"
embedding_cache = {}
if os.path.exists(embedding_cache_file):
    try:
        with open(embedding_cache_file, "rb") as f:
            embedding_cache = pickle.load(f)
    except (EOFError, pickle.UnpicklingError):
        print("Warning: Cache file is empty or corrupted. Starting with a fresh cache.")
        embedding_cache = {}

if __name__ == "__main__":
    while True:
        # Step 3: Capture user query
        user_query = input("Please enter your query below: (or type 'exit' to quit): ")
        
        # Exit condition
        if user_query.lower() == "exit":
            print("Goodbye!")
            break

        # Handle blank queries
        if not user_query.strip():
            print("Query cannot be blank. Please enter a valid query.")
            continue

        # Step 4: Use LLM to interpret the user's query
        interpreted_query = interpret_query(user_query)
        if "Could you please clarify what item youâ€™re looking for? This will help me assist you better." in interpreted_query:
            # Prompt user for clarification with examples
            print("Could you please specify the item you're looking for? For example, you could ask: 'Do you have apples?' or 'Is milk available in the grocery section?' This will help me provide the best assistance.")
            continue

        # Step 5: Use fuzzy matching to correct any spelling errors in the interpreted query
        corrected_item = correct_spelling(interpreted_query, inventory)
        if corrected_item != interpreted_query:
            # Ask user for confirmation of the corrected item
            user_confirmation = input(f"Did you mean: {corrected_item}? (yes/no): ").strip().lower()
            if user_confirmation != "yes":
                # If the user does not confirm, prompt them to rephrase their query
                print("Please provide more details or rephrase your query.")
                continue

        # Step 6: Determine the department of the corrected item
        department = determine_department(corrected_item, inventory)
        if department:
            # Display the department (optional logging for routing purposes)
            # print(f"Query routed to this department: {department}")
            print(department)
        else:
            # If department cannot be determined, ask the user to refine their query
            print("Could not determine the department. Please refine your query.")
            continue

        # Step 7: Generate an embedding for the query and retrieve results
        query_embedding = embed_query(f"{department} {corrected_item}", embedding_model)
        user_response = generate_user_response(query_embedding, inventory, index, user_query)

        # Step 8: Display the response generated by the LLM
        print(user_response)
